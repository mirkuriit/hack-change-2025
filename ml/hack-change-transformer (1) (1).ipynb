{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13921999,"sourceType":"datasetVersion","datasetId":8871457}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://dmiptrv0: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@github.com/mirkuriit/hack-change-2025.git","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-30T00:49:46.181922Z","iopub.execute_input":"2025-11-30T00:49:46.182196Z","iopub.status.idle":"2025-11-30T00:49:47.699233Z","shell.execute_reply.started":"2025-11-30T00:49:46.182172Z","shell.execute_reply":"2025-11-30T00:49:47.698360Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'hack-change-2025'...\nremote: Enumerating objects: 218, done.\u001b[K\nremote: Counting objects: 100% (218/218), done.\u001b[K\nremote: Compressing objects: 100% (146/146), done.\u001b[K\nremote: Total 218 (delta 86), reused 145 (delta 43), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (218/218), 799.89 KiB | 9.09 MiB/s, done.\nResolving deltas: 100% (86/86), done.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"cd hack-change-2025","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T00:49:50.486535Z","iopub.execute_input":"2025-11-30T00:49:50.486803Z","iopub.status.idle":"2025-11-30T00:49:50.492513Z","shell.execute_reply.started":"2025-11-30T00:49:50.486781Z","shell.execute_reply":"2025-11-30T00:49:50.491690Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/hack-change-2025\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!git checkout ml","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T00:49:52.978319Z","iopub.execute_input":"2025-11-30T00:49:52.978612Z","iopub.status.idle":"2025-11-30T00:49:53.115755Z","shell.execute_reply.started":"2025-11-30T00:49:52.978590Z","shell.execute_reply":"2025-11-30T00:49:53.114879Z"}},"outputs":[{"name":"stdout","text":"Branch 'ml' set up to track remote branch 'ml' from 'origin'.\nSwitched to a new branch 'ml'\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"\n!pip install transformers -q\n!pip install datasets -q\n!pip install nltk -q\nimport nltk\nnltk.download('stopwords')\n!pip install pymorphy3","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T00:49:54.710735Z","iopub.execute_input":"2025-11-30T00:49:54.711030Z","iopub.status.idle":"2025-11-30T00:50:13.750123Z","shell.execute_reply.started":"2025-11-30T00:49:54.711001Z","shell.execute_reply":"2025-11-30T00:50:13.749403Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"},{"name":"stdout","text":"Collecting pymorphy3\n  Downloading pymorphy3-2.0.6-py3-none-any.whl.metadata (2.4 kB)\nCollecting dawg2-python>=0.8.0 (from pymorphy3)\n  Downloading dawg2_python-0.9.0-py3-none-any.whl.metadata (7.5 kB)\nCollecting pymorphy3-dicts-ru (from pymorphy3)\n  Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl.metadata (2.0 kB)\nDownloading pymorphy3-2.0.6-py3-none-any.whl (53 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.9/53.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading dawg2_python-0.9.0-py3-none-any.whl (9.3 kB)\nDownloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl (8.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m112.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pymorphy3-dicts-ru, dawg2-python, pymorphy3\nSuccessfully installed dawg2-python-0.9.0 pymorphy3-2.0.6 pymorphy3-dicts-ru-2.4.417150.4580142\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install --upgrade pyarrow==14.0.2\nimport os\nos._exit(0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T00:50:18.710120Z","iopub.execute_input":"2025-11-30T00:50:18.711034Z","execution_failed":"2025-11-30T00:50:28.562Z"}},"outputs":[{"name":"stdout","text":"Collecting pyarrow==14.0.2\n  Downloading pyarrow-14.0.2-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\nRequirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.11/dist-packages (from pyarrow==14.0.2) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.6->pyarrow==14.0.2) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.6->pyarrow==14.0.2) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.6->pyarrow==14.0.2) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.6->pyarrow==14.0.2) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.6->pyarrow==14.0.2) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.6->pyarrow==14.0.2) (2.4.1)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.16.6->pyarrow==14.0.2) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.16.6->pyarrow==14.0.2) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.16.6->pyarrow==14.0.2) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.16.6->pyarrow==14.0.2) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.16.6->pyarrow==14.0.2) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.16.6->pyarrow==14.0.2) (2024.2.0)\nDownloading pyarrow-14.0.2-cp311-cp311-manylinux_2_28_x86_64.whl (38.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.0/38.0 MB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pyarrow\n  Attempting uninstall: pyarrow\n    Found existing installation: pyarrow 22.0.0\n    Uninstalling pyarrow-22.0.0:\n      Successfully uninstalled pyarrow-22.0.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ndatasets 4.4.1 requires pyarrow>=21.0.0, but you have pyarrow 14.0.2 which is incompatible.\nbigframes 2.12.0 requires pyarrow>=15.0.2, but you have pyarrow 14.0.2 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed pyarrow-14.0.2\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport matplotlib.ticker as ticker\nfrom transformers import get_scheduler\nimport matplotlib.pyplot as plt\n\nimport numpy as np\nimport pandas as pd\n\nimport random\nimport math\nimport time\nimport string\nimport re\n\nfrom pymorphy3 import MorphAnalyzer\n\n# datasets from huggingface\nfrom datasets import load_dataset\nfrom transformers import BertTokenizer, BertModel\n\nfrom nltk.corpus import stopwords\nimport nltk\n\nfrom tqdm.notebook import tqdm\nfrom torch.utils.tensorboard import SummaryWriter\nimport matplotlib.pyplot as plt\n\nimport os\nfrom datetime import datetime\nfrom torch.nn.utils.rnn import pad_sequence\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice\n\nimport gdown\n%matplotlib inline\n\nINPUT_DIR = \"/kaggle/input\"\nOUTPUT_DIR = \"/kaggle/output\"\n\nnltk.download('punkt')\nnltk.download('punkt_tab')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T00:51:07.528961Z","iopub.execute_input":"2025-11-30T00:51:07.529640Z","iopub.status.idle":"2025-11-30T00:51:07.540162Z","shell.execute_reply.started":"2025-11-30T00:51:07.529612Z","shell.execute_reply":"2025-11-30T00:51:07.539480Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\ndevice","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T00:51:08.984161Z","iopub.execute_input":"2025-11-30T00:51:08.985027Z","iopub.status.idle":"2025-11-30T00:51:08.990871Z","shell.execute_reply.started":"2025-11-30T00:51:08.984989Z","shell.execute_reply":"2025-11-30T00:51:08.990138Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"%load_ext tensorboard\n!rm -rf /content/gdrive/MyDrive/runs\n%tensorboard --logdir /content/gdrive/MyDrive/runs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T00:51:10.535088Z","iopub.execute_input":"2025-11-30T00:51:10.535391Z","iopub.status.idle":"2025-11-30T00:51:17.723897Z","shell.execute_reply.started":"2025-11-30T00:51:10.535367Z","shell.execute_reply":"2025-11-30T00:51:17.723148Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"\n        (async () => {\n            const url = new URL(await google.colab.kernel.proxyPort(6006, {'cache': true}));\n            url.searchParams.set('tensorboardColab', 'true');\n            const iframe = document.createElement('iframe');\n            iframe.src = url;\n            iframe.setAttribute('width', '100%');\n            iframe.setAttribute('height', '800');\n            iframe.setAttribute('frameborder', 0);\n            document.body.appendChild(iframe);\n        })();\n    "},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"BASE_FOLDER_PATH = '/kaggle/output'\nBERT_MODEL_NAME = 'bert-base-cased'\nBATCH_SIZE = 32\nEPOCHS = 1\nMODEL_NAME = 'model_v0'\nLR = 0.00005\nN_HEADS = 1\nTRAIN_SIZE = 20000\nTEST_SIZE = 1000\n\nMODEL_FOLDER_PATH = os.path.join(BASE_FOLDER_PATH, MODEL_NAME)\n\nSCHEDULER_LAMBDA_PARAM = 0.96\nPAD_IND = 0\nHIDDEN_DIM = 768\n\nENGLISH_STOP_WORDS = set(stopwords.words('english'))\nPUNCT_WORD_TOKENIZER = nltk.WordPunctTokenizer() # for preprocess\n# for broadening your horizons use it for lemmatization\nMORPH_ANALYZER = MorphAnalyzer()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T00:51:17.725219Z","iopub.execute_input":"2025-11-30T00:51:17.725492Z","iopub.status.idle":"2025-11-30T00:51:17.814228Z","shell.execute_reply.started":"2025-11-30T00:51:17.725462Z","shell.execute_reply":"2025-11-30T00:51:17.813668Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"os.makedirs(MODEL_FOLDER_PATH, exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T00:51:25.310812Z","iopub.execute_input":"2025-11-30T00:51:25.311373Z","iopub.status.idle":"2025-11-30T00:51:25.315558Z","shell.execute_reply.started":"2025-11-30T00:51:25.311345Z","shell.execute_reply":"2025-11-30T00:51:25.314776Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"train_data = pd.read_csv(f\"{INPUT_DIR}/hack-change/train.csv\").to_dict(orient=\"records\")\ntest_data  = pd.read_csv(f\"{INPUT_DIR}/hack-change/test.csv\").to_dict(orient=\"records\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T00:51:26.205292Z","iopub.execute_input":"2025-11-30T00:51:26.205858Z","iopub.status.idle":"2025-11-30T00:51:30.413800Z","shell.execute_reply.started":"2025-11-30T00:51:26.205834Z","shell.execute_reply":"2025-11-30T00:51:30.413172Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def preprocess_text(text):\n    nums_filtered_text = re.sub(r'[0-9]+', '', text.lower())\n    punct_filtered_text = ''.join(\n        [ch for ch in nums_filtered_text if ch not in string.punctuation]\n    )\n    tokens = PUNCT_WORD_TOKENIZER.tokenize(punct_filtered_text)\n    filtr_stop_words_tokens = [MORPH_ANALYZER.parse(token)[0].normal_form for token in tokens\n                             if token not in ENGLISH_STOP_WORDS]\n    norm_tokens = [MORPH_ANALYZER.parse(token)[0].normal_form for token in filtr_stop_words_tokens]\n\n    return f\"[CLS] {' '.join(norm_tokens)}\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T00:51:30.415025Z","iopub.execute_input":"2025-11-30T00:51:30.415326Z","iopub.status.idle":"2025-11-30T00:51:30.420358Z","shell.execute_reply.started":"2025-11-30T00:51:30.415295Z","shell.execute_reply":"2025-11-30T00:51:30.419818Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def prepare_bert_tokenizer_and_embedder(bert_model_name, device=device):\n    tokenizer = BertTokenizer.from_pretrained(\n        bert_model_name,\n        model_max_length=512,\n        truncation=True\n    )\n\n    bert = BertModel.from_pretrained(bert_model_name)\n\n    # Удаляем encoder ПРАВИЛЬНО — пока модель на CPU\n    bert.pooler = nn.Identity()\n    bert.encoder.layer = nn.ModuleList([])\n\n    # Теперь переносим всю модель на GPU\n    bert = bert.to(device)\n\n    return tokenizer, bert\n\n\ntokenizer, embedder = prepare_bert_tokenizer_and_embedder(bert_model_name=BERT_MODEL_NAME)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T00:51:33.238916Z","iopub.execute_input":"2025-11-30T00:51:33.239465Z","iopub.status.idle":"2025-11-30T00:51:39.874798Z","shell.execute_reply.started":"2025-11-30T00:51:33.239418Z","shell.execute_reply":"2025-11-30T00:51:39.873923Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c66a637dd974712b6df2861da750c53"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4021eaa944b94e6991008f80291a8694"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72e6dba22109473bac79f48e0e6cd8ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46ae1f88d7a444189af6ed2057bbb71b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37cffa17ff4b4afc99a4f9c5a81ed0b7"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"lengs = [len(tokenizer.tokenize(train_json['text'])) for train_json in tqdm(train_data)]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T18:08:15.538425Z","iopub.execute_input":"2025-11-29T18:08:15.539136Z","iopub.status.idle":"2025-11-29T18:12:05.232942Z","shell.execute_reply.started":"2025-11-29T18:08:15.539107Z","shell.execute_reply":"2025-11-29T18:12:05.232028Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/232366 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e42363db1214c9abe5ade3419f3a7f6"}},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"plt.hist(lengs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T18:12:06.419951Z","iopub.execute_input":"2025-11-29T18:12:06.420697Z","iopub.status.idle":"2025-11-29T18:12:07.030673Z","shell.execute_reply.started":"2025-11-29T18:12:06.420668Z","shell.execute_reply":"2025-11-29T18:12:07.030057Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"(array([108173.,  42580.,  21902.,  14260.,  11218.,   9312.,   8172.,\n          7033.,   6301.,   3415.]),\n array([   0. ,  129.5,  259. ,  388.5,  518. ,  647.5,  777. ,  906.5,\n        1036. , 1165.5, 1295. ]),\n <BarContainer object of 10 artists>)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"def length_to_mask(length, max_len=None, dtype=None):\n\n    assert len(length.shape) == 1\n    max_len = max_len or length.max().item()\n    mask = torch.arange(max_len, device=length.device,\n                        dtype=length.dtype).expand(len(length), max_len) < length.unsqueeze(1)\n    if dtype is not None:\n        mask = torch.as_tensor(mask, dtype=dtype, device=length.device)\n    return mask","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T00:51:51.933111Z","iopub.execute_input":"2025-11-30T00:51:51.933400Z","iopub.status.idle":"2025-11-30T00:51:51.938143Z","shell.execute_reply.started":"2025-11-30T00:51:51.933379Z","shell.execute_reply":"2025-11-30T00:51:51.937341Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"class MyDataset(Dataset):\n  def __init__(self, dataset, tokenizer, embedder=None, device=device):\n    self.dataset = dataset # hugging dataset object\n    self.tokenizer = tokenizer\n    self.embedder = embedder\n    self.device = device\n\n  def __getitem__(self, idx):\n    item_dict = self.dataset[idx]\n    text = str(item_dict[\"text\"])\n    target = torch.tensor(item_dict[\"label\"], dtype=torch.long)\n    \n    normalized_text = preprocess_text(text)\n    \n    encoded = self.tokenizer(\n        normalized_text,\n        truncation=True,\n        max_length=512,\n        padding=False,\n        return_tensors=\"pt\"\n    )\n    token_ids = encoded[\"input_ids\"].squeeze(0)\n    length = token_ids.size(0)\n    \n    if self.embedder is not None:\n        with torch.no_grad():\n            embed_output = self.embedder(token_ids.unsqueeze(0).to(self.device))\n        return embed_output, target, length\n    else:\n        return token_ids, target, length\n\n\n\n  def text_to_tokens_ids(self, text):\n    tokens = self.tokenizer.tokenize(text)\n\n    return torch.tensor(self.tokenizer.convert_tokens_to_ids(tokens)).to(device)\n\n  def __len__(self):\n    return len(self.dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T00:51:52.457726Z","iopub.execute_input":"2025-11-30T00:51:52.458268Z","iopub.status.idle":"2025-11-30T00:51:52.464466Z","shell.execute_reply.started":"2025-11-30T00:51:52.458241Z","shell.execute_reply":"2025-11-30T00:51:52.463810Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def collate_batch(batch):\n  targets_list, embeddings_list, lengths_list = [], [], []\n\n  for (_embed_output, _target, _text_len_in_tokens) in batch:\n    _embed = _embed_output.last_hidden_state\n    targets_list.append(_target)\n    embeddings_list.append(_embed[0])\n    lengths_list.append(_text_len_in_tokens)\n\n  targets_tensor = torch.tensor(targets_list, dtype=torch.int64).to(device)\n  embeddings_tensor = pad_sequence(embeddings_list, batch_first=True, padding_value=PAD_IND).to(device)\n  lengths_tensor = torch.tensor(lengths_list, dtype=torch.int64).to(device)\n\n  return embeddings_tensor.detach(), targets_tensor.detach(), lengths_tensor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T00:51:53.083747Z","iopub.execute_input":"2025-11-30T00:51:53.084014Z","iopub.status.idle":"2025-11-30T00:51:53.089299Z","shell.execute_reply.started":"2025-11-30T00:51:53.083994Z","shell.execute_reply":"2025-11-30T00:51:53.088498Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_val = train_test_split(train_data, test_size=0.1, shuffle=False)\n\ntrain_dataset = MyDataset(X_train, tokenizer=tokenizer, embedder=embedder)\ntest_dataset = MyDataset(X_val, tokenizer=tokenizer, embedder=embedder)\n\ntrain_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE, collate_fn=collate_batch, drop_last=True)\ntest_dataloader = DataLoader(test_dataset, shuffle=False, batch_size=3, collate_fn=collate_batch, drop_last=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T00:51:54.131697Z","iopub.execute_input":"2025-11-30T00:51:54.132185Z","iopub.status.idle":"2025-11-30T00:51:54.161180Z","shell.execute_reply.started":"2025-11-30T00:51:54.132159Z","shell.execute_reply":"2025-11-30T00:51:54.160360Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"sentence_samples_to_show_attention = [train_data[1129], train_data[4534]]\nsentence_samples_to_show_attention = [sent[1] for sent in sentence_samples_to_show_attention]\n\nprint(sentence_samples_to_show_attention)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T20:44:51.596920Z","iopub.execute_input":"2025-11-29T20:44:51.597229Z","iopub.status.idle":"2025-11-29T20:44:51.833715Z","shell.execute_reply.started":"2025-11-29T20:44:51.597207Z","shell.execute_reply":"2025-11-29T20:44:51.832688Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_151/1807432236.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msentence_samples_to_show_attention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1129\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4534\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msentence_samples_to_show_attention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence_samples_to_show_attention\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_samples_to_show_attention\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_151/1807432236.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msentence_samples_to_show_attention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1129\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4534\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msentence_samples_to_show_attention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence_samples_to_show_attention\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_samples_to_show_attention\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 1"],"ename":"KeyError","evalue":"1","output_type":"error"}],"execution_count":14},{"cell_type":"code","source":"class MultiHeadAttentionLayer(nn.Module):\n    def __init__(self, hid_dim, n_heads, dropout, device):\n        super().__init__()\n\n        assert hid_dim % n_heads == 0\n\n        self.hid_dim = hid_dim\n        self.n_heads = n_heads\n        self.head_dim = hid_dim // n_heads\n\n        self.fc_q = nn.Linear(hid_dim, hid_dim)\n        self.fc_k = nn.Linear(hid_dim, hid_dim)\n        self.fc_v = nn.Linear(hid_dim, hid_dim)\n\n        self.fc_o = nn.Linear(hid_dim, hid_dim)\n\n        self.dropout = nn.Dropout(dropout)\n\n        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n\n    def forward(self, query, key, value, mask = None):\n\n        batch_size = query.shape[0]\n\n        #query = [batch size, query len, hid dim]\n        #key = [batch size, key len, hid dim]\n        #value = [batch size, value len, hid dim]\n\n        Q = self.fc_q(query)\n        K = self.fc_k(key)\n        V = self.fc_v(value)\n\n        #Q = [batch size, query len, hid dim]\n        #K = [batch size, key len, hid dim]\n        #V = [batch size, value len, hid dim]\n\n        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n\n        #Q = [batch size, n heads, query len, head dim]\n        #K = [batch size, n heads, key len, head dim]\n        #V = [batch size, n heads, value len, head dim]\n\n        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n\n        #energy = [batch size, n heads, query len, key len]\n\n        if mask is not None:\n            mask = mask[:, None, None, :]\n            energy = energy.masked_fill(mask == 0, -1e10)\n\n        attention = torch.softmax(energy, dim = -1)\n\n        #attention = [batch size, n heads, query len, key len]\n\n        x = torch.matmul(self.dropout(attention), V)\n\n        #x = [batch size, n heads, query len, head dim]\n\n        x = x.permute(0, 2, 1, 3).contiguous()\n\n        #x = [batch size, query len, n heads, head dim]\n\n        x = x.view(batch_size, -1, self.hid_dim)\n\n        #x = [batch size, query len, hid dim]\n\n        x = self.fc_o(x)\n\n        #x = [batch size, query len, hid dim]\n\n        return x, attention","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T20:44:52.345163Z","iopub.execute_input":"2025-11-29T20:44:52.345532Z","iopub.status.idle":"2025-11-29T20:44:52.357984Z","shell.execute_reply.started":"2025-11-29T20:44:52.345507Z","shell.execute_reply":"2025-11-29T20:44:52.357325Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"class SelfAttentionBasedClassifier(nn.Module):\n  def __init__(self, hid_dim, cnt_class=3, device=device, n_heads=N_HEADS):\n    super().__init__()\n    self.hid_dim = hid_dim\n    self.device = device\n    self.cnt_class = cnt_class\n    self.attn = MultiHeadAttentionLayer(hid_dim=self.hid_dim, n_heads=n_heads, dropout=0, device=self.device)\n    self.classifier_head = nn.Linear(self.hid_dim, self.cnt_class)\n    self.softmax = nn.Softmax(dim=-1)\n\n  def forward(self, x, mask = None):\n    x, attention = self.attn(x, x, x, mask=mask)\n    # let's classifier by 0 token information\n    # Reason of 0 token\n    x = x[:, 0, :].squeeze()\n    x = self.classifier_head(x)\n\n    #return self.softmax(x), attention\n    return x, attention","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T20:44:53.444631Z","iopub.execute_input":"2025-11-29T20:44:53.444947Z","iopub.status.idle":"2025-11-29T20:44:53.450126Z","shell.execute_reply.started":"2025-11-29T20:44:53.444925Z","shell.execute_reply":"2025-11-29T20:44:53.449411Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"model = SelfAttentionBasedClassifier(HIDDEN_DIM).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T18:12:12.164080Z","iopub.execute_input":"2025-11-29T18:12:12.164801Z","iopub.status.idle":"2025-11-29T18:12:12.190661Z","shell.execute_reply.started":"2025-11-29T18:12:12.164778Z","shell.execute_reply":"2025-11-29T18:12:12.190123Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"loss_fn = torch.nn.CrossEntropyLoss()\ntimestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\noptimizer = optim.Adam(model.parameters(), lr=LR)\n\nlambda_scheduler = lambda x: SCHEDULER_LAMBDA_PARAM ** x\nscheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_scheduler)\n\nwriter = SummaryWriter(os.path.join(BASE_FOLDER_PATH, 'runs/{}'.format(MODEL_NAME)))\n\ndef train_one_epoch(epoch_index, model, training_loader, scheduler, optimizer, loss_fn, tb_writer):\n    running_loss = 0.\n    last_loss = 0.\n    train_loss = 0.\n\n    i = 0\n    cnt_right_answers = 0\n    cnt_answers = 0\n\n    all_answers = []\n    answers_probs = []\n    all_labels = []\n    for data in tqdm(training_loader):\n        input_data, labels, lengths = data\n        mask = length_to_mask(lengths)\n\n        if epoch_index != -1:\n            optimizer.zero_grad()\n        input_data = input_data.to(device)\n        labels = labels.to(device).long()\n        outputs, attention = model(input_data, mask=mask)\n\n        answers = outputs.argmax(axis=-1)\n        loss = loss_fn(outputs, labels)\n        if epoch_index != -1:\n            loss.backward()\n            optimizer.step()\n\n        running_loss += loss.item()\n        train_loss += loss.item()\n        if i % 10 == 9 and epoch_index != -1:\n            last_loss = running_loss / 10 # loss per batch\n            tb_x = epoch_index * len(training_loader) + i + 1\n            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n            running_loss = 0.\n\n            if i % 10 == 0 and scheduler is not None:\n                tb_writer.add_scalar('Scheduler LR', optimizer.param_groups[0][\"lr\"], tb_x)\n                scheduler.step()\n            \n\n        cnt_answers += labels.shape[0]\n        cnt_right_answers += (answers == labels).sum().item()\n\n        i += 1\n\n    train_loss = train_loss / len(training_loader)\n    tb_writer.add_scalar('Accuracy/train', cnt_right_answers / cnt_answers, epoch_index + 1)\n\n    return train_loss\n\n\ndef validation(epoch_number, val_dataloader):\n    val_loss = 0.0\n\n    all_answers = []\n    all_labels = []\n\n    cnt_answers = 0\n    cnt_right_answers = 0\n    model.eval()\n    with torch.no_grad():\n        for i, vdata in tqdm(enumerate(val_dataloader), total=len(val_dataloader)):\n            input_data, labels, lengths = vdata\n            mask = length_to_mask(lengths)\n\n            input_data = input_data.to(device)\n            labels = labels.to(device).long()\n            outputs, attention = model(input_data, mask=mask)\n\n            vloss = loss_fn(outputs, labels)\n            val_loss += vloss.item()\n\n            answers = outputs.argmax(dim=-1)\n\n            all_answers.extend(answers.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n            cnt_answers += labels.shape[0]\n            cnt_right_answers += (answers == labels).sum().item()\n\n            del input_data, labels, lengths\n\n    val_loss = val_loss / len(val_dataloader)\n    val_acc = cnt_right_answers / cnt_answers\n\n    # F1 Macro\n    f1_macro = f1_score(all_labels, all_answers, average='macro')\n\n    print(f\"VAL LOSS = {val_loss:.4f}, ACC = {val_acc:.4f}, F1 Macro = {f1_macro:.4f}\")\n    return val_loss, val_acc, f1_macro","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T18:12:19.099515Z","iopub.execute_input":"2025-11-29T18:12:19.100051Z","iopub.status.idle":"2025-11-29T18:12:19.117227Z","shell.execute_reply.started":"2025-11-29T18:12:19.100024Z","shell.execute_reply":"2025-11-29T18:12:19.116613Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"best_vloss = 1_000_000.\n\nfor epoch_number in tqdm(range(EPOCHS)):\n    print('EPOCH {}:'.format(epoch_number + 1))\n\n    model.train()\n    train_loss = train_one_epoch(\n        epoch_index=epoch_number,\n        model=model,\n        training_loader=train_dataloader,\n        optimizer=optimizer,\n        scheduler=scheduler,\n        loss_fn=loss_fn,\n        tb_writer=writer)\n\n    model.eval()\n    val_loss, val_acc, f1_macro = validation(epoch_number, test_dataloader)\n\n    writer.add_scalar('Loss/valid', val_loss, epoch_number + 1)\n    writer.add_scalar('Accuracy/valid', val_acc, epoch_number + 1)\n    writer.add_scalar(\"F1-macro\", f1_macro, epoch_number + 1)\n    if val_loss < best_vloss:\n        best_vloss = val_loss\n        model_path = os.path.join(MODEL_FOLDER_PATH, 'model_{}_{}'.format(epoch_number + 1, timestamp))\n        torch.save(model.state_dict(), model_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T17:48:44.391783Z","iopub.execute_input":"2025-11-29T17:48:44.392076Z","iopub.status.idle":"2025-11-29T17:57:24.790976Z","shell.execute_reply.started":"2025-11-29T17:48:44.392054Z","shell.execute_reply":"2025-11-29T17:57:24.789868Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7230ae82c97041abbfd2ff4769aad297"}},"metadata":{}},{"name":"stdout","text":"EPOCH 1:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/6535 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"522a9194087c49119c421e897900085a"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_130/336135885.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     train_loss = train_one_epoch(\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mepoch_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch_number\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_130/2667197317.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(epoch_index, model, training_loader, scheduler, optimizer, loss_fn, tb_writer)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0manswers_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mall_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0minput_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlength_to_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_130/94052098.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mnormalized_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     encoded = self.tokenizer(\n","\u001b[0;32m/tmp/ipykernel_130/3395620960.py\u001b[0m in \u001b[0;36mpreprocess_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      5\u001b[0m     )\n\u001b[1;32m      6\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPUNCT_WORD_TOKENIZER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpunct_filtered_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     filtr_stop_words_tokens = [MORPH_ANALYZER.parse(token)[0].normal_form for token in tokens\n\u001b[0m\u001b[1;32m      8\u001b[0m                              if token not in ENGLISH_STOP_WORDS]\n\u001b[1;32m      9\u001b[0m     \u001b[0mnorm_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mMORPH_ANALYZER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal_form\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiltr_stop_words_tokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_130/3395620960.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      5\u001b[0m     )\n\u001b[1;32m      6\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPUNCT_WORD_TOKENIZER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpunct_filtered_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     filtr_stop_words_tokens = [MORPH_ANALYZER.parse(token)[0].normal_form for token in tokens\n\u001b[0m\u001b[1;32m      8\u001b[0m                              if token not in ENGLISH_STOP_WORDS]\n\u001b[1;32m      9\u001b[0m     \u001b[0mnorm_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mMORPH_ANALYZER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal_form\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiltr_stop_words_tokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pymorphy3/analyzer.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprob_estimator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprob_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_to_parses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_lower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result_type\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pymorphy3/analyzer.py\u001b[0m in \u001b[0;36mapply_to_parses\u001b[0;34m(self, word, word_lower, parses)\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         probs = [\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp_t_given_w\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_lower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormal_form\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethods_stack\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pymorphy3/analyzer.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         probs = [\n\u001b[0;32m---> 80\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp_t_given_w\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_lower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormal_form\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethods_stack\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         ]\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pymorphy3/dawg.py\u001b[0m in \u001b[0;36mprob\u001b[0;34m(self, word, tag)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mdawg_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{word}:{tag}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdawg_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMULTIPLIER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/dawg_python/dawgs.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mLOOKUP_ERROR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/dawg_python/dawgs.py\u001b[0m in \u001b[0;36mb_get_value\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mb_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/dawg_python/wrapper.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;34m\"\"\"Exact matching (returns value)\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfollow_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mROOT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/dawg_python/wrapper.py\u001b[0m in \u001b[0;36mfollow_bytes\u001b[0;34m(self, s, index)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;34m\"\"\"Follows transitions.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfollow_char\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/dawg_python/wrapper.py\u001b[0m in \u001b[0;36mfollow_char\u001b[0;34m(self, label, index)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mnext_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m^\u001b[0m \u001b[0moffset\u001b[0m \u001b[0;34m^\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0munits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPRECISION_MASK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0munits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_units\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/dawg_python/units.py\u001b[0m in \u001b[0;36mlabel\u001b[0;34m(base, _mask)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIS_LEAF_BIT\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;36m0xFF\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0;34m\"\"\"Read a label with a leaf flag from a non-leaf unit.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbase\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0m_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":158},{"cell_type":"code","source":"# 1️⃣ Текст\ntext = \"Слева от меня Алексей Ильмухин. Путешественник и интересный человек. В числе его личных побед - путешествие на велосипеде от Челябинска до Байкала... Приехал к нам помочь в организации семинара Мухтара. Просто взял и приехал из Челябинска и начал помогать... ))👍 #можеткаждый\"\npreprocessed_text = preprocess_text(text)\n\n# 2️⃣ Токенизация\nencoded = tokenizer(\n    preprocessed_text,\n    truncation=True,\n    max_length=512,\n    padding='max_length',\n    return_tensors=\"pt\"\n)\n\ninput_ids = encoded[\"input_ids\"].to(device)\nattention_mask = encoded[\"attention_mask\"].to(device)\n\n# 3️⃣ Получаем embeddings через отдельный embedder\nwith torch.no_grad():\n    embed_output = embedder(input_ids=input_ids, attention_mask=attention_mask)\n    embeddings = embed_output.last_hidden_state  # float, подходящий для attention\n\n# 4️⃣ Forward через SelfAttentionBasedClassifier\nmodel.eval()\nwith torch.no_grad():\n    outputs, attention = model(embeddings, mask=attention_mask)\n\n# 5️⃣ Предсказание\npredicted_class = outputs\nprint(f\"Предсказанный класс: {predicted_class}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T18:12:25.357217Z","iopub.execute_input":"2025-11-29T18:12:25.357807Z","iopub.status.idle":"2025-11-29T18:12:25.665789Z","shell.execute_reply.started":"2025-11-29T18:12:25.357783Z","shell.execute_reply":"2025-11-29T18:12:25.664998Z"}},"outputs":[{"name":"stdout","text":"Предсказанный класс: tensor([-0.0159, -0.0180,  0.0543], device='cuda:0')\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"print(train_data[0])","metadata":{"execution":{"iopub.status.busy":"2025-11-29T18:03:39.626804Z","iopub.execute_input":"2025-11-29T18:03:39.627144Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h1> BERT PRETRAINED</h1>","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification, AutoTokenizer\nfrom tqdm.auto import tqdm\nfrom peft import LoraConfig, get_peft_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T07:16:03.354382Z","iopub.execute_input":"2025-11-30T07:16:03.354692Z","iopub.status.idle":"2025-11-30T07:16:03.726714Z","shell.execute_reply.started":"2025-11-30T07:16:03.354672Z","shell.execute_reply":"2025-11-30T07:16:03.726136Z"}},"outputs":[],"execution_count":234},{"cell_type":"code","source":"del model, tokenizer, optimizer, scheduler  # delete our previous objects","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T20:11:54.377589Z","iopub.execute_input":"2025-11-29T20:11:54.377887Z","iopub.status.idle":"2025-11-29T20:11:54.614174Z","shell.execute_reply.started":"2025-11-29T20:11:54.377866Z","shell.execute_reply":"2025-11-29T20:11:54.613078Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_171/1656625710.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m  \u001b[0;31m# delete our previous objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"],"ename":"NameError","evalue":"name 'model' is not defined","output_type":"error"}],"execution_count":16},{"cell_type":"code","source":"NUM_LABELS = 3\nBATCH_SIZE = 8\nMAX_LENGTH = 512","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# download pretrained models\ntokenizer = AutoTokenizer.from_pretrained(\"DeepPavlov/rubert-base-cased\")\nbase_model  = AutoModelForSequenceClassification.from_pretrained(\"DeepPavlov/rubert-base-cased\", num_labels=3).to(device)\n\nlora_config = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    target_modules=\"all-linear\",   # применить ко всем линейным слоям\n    lora_dropout=0.05,\n    bias=\"none\"\n)\n\nmodel = get_peft_model(base_model, lora_config)\n\n# Заморозка всех слоёв BERT\nfor param in model.bert.parameters():\n    param.requires_grad = False\n\n# Только классификационный слой обучаем\nfor param in model.classifier.parameters():\n    param.requires_grad = True\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-30T07:36:29.719Z"}},"outputs":[],"execution_count":null},{"cell_type":"raw","source":"print(model)","metadata":{}},{"cell_type":"code","source":"for param in model.bert.parameters():\n    param.requires_grad = False\n\nfor i in range(13, 12):\n    for param in model.bert.encoder.layer[i].parameters():\n        param.requires_grad = True\n\nfor param in model.classifier.parameters():\n    param.requires_grad = True\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T07:34:09.031526Z","iopub.execute_input":"2025-11-30T07:34:09.032052Z","iopub.status.idle":"2025-11-30T07:34:09.037158Z","shell.execute_reply.started":"2025-11-30T07:34:09.032025Z","shell.execute_reply":"2025-11-30T07:34:09.036389Z"}},"outputs":[],"execution_count":253},{"cell_type":"code","source":"from tqdm.auto import tqdm\nimport torch\nfrom sklearn.model_selection import train_test_split\n\ndef tokenize_dataset(data_list, tokenizer, max_length=MAX_LENGTH):\n    input_ids_list = []\n    attention_mask_list = []\n    labels_list = []\n\n    for example in tqdm(data_list, desc=\"Tokenizing\"):\n        encoded = tokenizer(\n            example[\"text\"],\n            padding=\"max_length\",\n            truncation=True,\n            max_length=max_length,\n            return_tensors=\"pt\"\n        )\n        # НЕ делаем squeeze, оставляем размерность (1, seq_len)\n        input_ids_list.append(encoded[\"input_ids\"])\n        attention_mask_list.append(encoded[\"attention_mask\"])\n        labels_list.append(torch.tensor(example[\"label\"], dtype=torch.long).unsqueeze(0))\n\n    # Конкатенируем по первой размерности, чтобы получить (num_examples, seq_len)\n    return {\n        \"input_ids\": torch.cat(input_ids_list, dim=0),\n        \"attention_mask\": torch.cat(attention_mask_list, dim=0),\n        \"labels\": torch.cat(labels_list, dim=0)\n    }\n\n# Разделяем данные\nX_train, X_val = train_test_split(train_data, test_size=0.1, shuffle=False, random_state=42)\n\n# Токенизируем с передачей tokenizer\ntokenized_train_dataset = tokenize_dataset(X_train, tokenizer, max_length=MAX_LENGTH)\ntokenized_val_dataset = tokenize_dataset(X_val, tokenizer, max_length=MAX_LENGTH)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T00:52:33.355632Z","iopub.execute_input":"2025-11-30T00:52:33.356214Z","iopub.status.idle":"2025-11-30T00:55:43.013529Z","shell.execute_reply.started":"2025-11-30T00:52:33.356182Z","shell.execute_reply":"2025-11-30T00:55:43.012833Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Tokenizing:   0%|          | 0/209129 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2fab4db740894096b625d35b50eb5e02"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing:   0%|          | 0/23237 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c89f5210c504f5f947e5f00b202d992"}},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"from torch.utils.data import TensorDataset, DataLoader\n\ntrain_dataset = TensorDataset(\n    tokenized_train_dataset[\"input_ids\"],\n    tokenized_train_dataset[\"attention_mask\"],\n    tokenized_train_dataset[\"labels\"]\n)\nval_dataset = TensorDataset(\n    tokenized_val_dataset[\"input_ids\"],\n    tokenized_val_dataset[\"attention_mask\"],\n    tokenized_val_dataset[\"labels\"]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T00:55:45.308640Z","iopub.execute_input":"2025-11-30T00:55:45.309362Z","iopub.status.idle":"2025-11-30T00:55:45.313331Z","shell.execute_reply.started":"2025-11-30T00:55:45.309338Z","shell.execute_reply":"2025-11-30T00:55:45.312692Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\nval_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T01:45:24.965171Z","iopub.execute_input":"2025-11-30T01:45:24.965954Z","iopub.status.idle":"2025-11-30T01:45:24.969730Z","shell.execute_reply.started":"2025-11-30T01:45:24.965927Z","shell.execute_reply":"2025-11-30T01:45:24.968973Z"}},"outputs":[],"execution_count":53},{"cell_type":"code","source":"def save_checkpoint(model, epoch, optimizer, scheduler, loss):\n    checkpoint_path = f\"checkpoint_step_{global_step}.pt\"\n    torch.save({\n        'global_step': global_step,\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'scheduler_state_dict': scheduler.state_dict(),\n        'loss': loss.item()\n    }, checkpoint_path)\n    print(f\"\\nCheckpoint saved at step {global_step} → {checkpoint_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T00:55:52.730032Z","iopub.execute_input":"2025-11-30T00:55:52.730324Z","iopub.status.idle":"2025-11-30T00:55:52.735004Z","shell.execute_reply.started":"2025-11-30T00:55:52.730302Z","shell.execute_reply":"2025-11-30T00:55:52.734324Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"global_step = 0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T00:57:00.385649Z","iopub.execute_input":"2025-11-30T00:57:00.386226Z","iopub.status.idle":"2025-11-30T00:57:00.389355Z","shell.execute_reply.started":"2025-11-30T00:57:00.386199Z","shell.execute_reply":"2025-11-30T00:57:00.388784Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"ls","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T05:24:56.980172Z","iopub.execute_input":"2025-11-30T05:24:56.980847Z","iopub.status.idle":"2025-11-30T05:24:57.349662Z","shell.execute_reply.started":"2025-11-30T05:24:56.980814Z","shell.execute_reply":"2025-11-30T05:24:57.348672Z"}},"outputs":[{"name":"stdout","text":"checkpoint_step_120000.pt  checkpoint_step_420000.pt  \u001b[0m\u001b[01;34mkaggle\u001b[0m/\ncheckpoint_step_180000.pt  checkpoint_step_480000.pt  \u001b[01;34mrubert_local\u001b[0m/\ncheckpoint_step_240000.pt  checkpoint_step_540000.pt  save_data_new.pkl\ncheckpoint_step_300000.pt  full_model_weights.pt\ncheckpoint_step_360000.pt  \u001b[01;34mhack-change-2025\u001b[0m/\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":153},{"cell_type":"code","source":"save_steps = 30000","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T01:45:05.676538Z","iopub.execute_input":"2025-11-30T01:45:05.677203Z","iopub.status.idle":"2025-11-30T01:45:05.680736Z","shell.execute_reply.started":"2025-11-30T01:45:05.677180Z","shell.execute_reply":"2025-11-30T01:45:05.679894Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"checkpoint = torch.load('checkpoint_step_0.pt', map_location=device)\n\nmodel.load_state_dict(checkpoint['model_state_dict'])\noptimizer.load_state_dict(checkpoint['optimizer_state_dict'])\nscheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n\nglobal_step = checkpoint['global_step']\nstart_epoch = checkpoint['epoch']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T00:18:13.740075Z","iopub.execute_input":"2025-11-30T00:18:13.740371Z","iopub.status.idle":"2025-11-30T00:18:13.783449Z","shell.execute_reply.started":"2025-11-30T00:18:13.740345Z","shell.execute_reply":"2025-11-30T00:18:13.782488Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_133/3077885451.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'checkpoint_step_0.pt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'optimizer_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'scheduler_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1423\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1425\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1426\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1427\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    749\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"w\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'checkpoint_step_0.pt'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'checkpoint_step_0.pt'","output_type":"error"}],"execution_count":35},{"cell_type":"code","source":"!pip install nlpaug\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T02:56:41.121357Z","iopub.execute_input":"2025-11-30T02:56:41.122085Z","iopub.status.idle":"2025-11-30T02:56:45.272494Z","shell.execute_reply.started":"2025-11-30T02:56:41.122062Z","shell.execute_reply":"2025-11-30T02:56:45.271739Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting nlpaug\n  Downloading nlpaug-1.1.11-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.11/dist-packages (from nlpaug) (1.26.4)\nRequirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from nlpaug) (2.2.3)\nRequirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.11/dist-packages (from nlpaug) (2.32.5)\nRequirement already satisfied: gdown>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from nlpaug) (5.2.0)\nRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown>=4.0.0->nlpaug) (4.13.4)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown>=4.0.0->nlpaug) (3.20.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown>=4.0.0->nlpaug) (4.67.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.2->nlpaug) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.2->nlpaug) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.2->nlpaug) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.2->nlpaug) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.2->nlpaug) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.2->nlpaug) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.0->nlpaug) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.0->nlpaug) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.0->nlpaug) (2025.2)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->nlpaug) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->nlpaug) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->nlpaug) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->nlpaug) (2025.10.5)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.2.0->nlpaug) (1.17.0)\nRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.7)\nRequirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (4.15.0)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.16.2->nlpaug) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.16.2->nlpaug) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.16.2->nlpaug) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.16.2->nlpaug) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.16.2->nlpaug) (2024.2.0)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown>=4.0.0->nlpaug) (1.7.1)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.16.2->nlpaug) (2024.2.0)\nDownloading nlpaug-1.1.11-py3-none-any.whl (410 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.5/410.5 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: nlpaug\nSuccessfully installed nlpaug-1.1.11\n","output_type":"stream"}],"execution_count":88},{"cell_type":"code","source":"import nltk\nnltk.download('averaged_perceptron_tagger_eng')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T03:02:10.999470Z","iopub.execute_input":"2025-11-30T03:02:10.999774Z","iopub.status.idle":"2025-11-30T03:02:11.061036Z","shell.execute_reply.started":"2025-11-30T03:02:10.999754Z","shell.execute_reply":"2025-11-30T03:02:11.060476Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n","output_type":"stream"},{"execution_count":99,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":99},{"cell_type":"code","source":"from tqdm.auto import tqdm\nimport nlpaug.augmenter.word as naw\ntext_augmenter = naw.SynonymAug(aug_src='wordnet', lang='eng')\n\ndef augment_text(text, augmenter, n=2):\n    \"\"\"\n    Генерирует n аугментированных версий текста.\n    \"\"\"\n    return [augmenter.augment(text) for _ in range(n)]\n\ndef augment_data_with_structure(train_data, augmentation_factor=2):\n    \"\"\"\n    Аугментация текстов с сохранением структуры исходных данных.\n    \n    train_data: список словарей с ключами 'ID', 'text', 'src', 'label'\n    augmentation_factor: сколько аугментированных версий создавать для каждого текста\n    \"\"\"\n    augmented_data = []\n\n    for row in tqdm(train_data, desc=\"Augmenting data\"):\n        original_text = row['text']\n        label = row['label']\n        idx = row['ID']\n        source = row.get('src', 'original')\n\n        # Генерируем аугментированные тексты\n        aug_texts = augment_text(original_text, text_augmenter, augmentation_factor)\n\n        for j, aug_text in enumerate(aug_texts):\n            new_id = f\"{idx}_aug_{j}\"\n            new_src = f\"{source}_augmented\"\n\n            augmented_row = {\n                'ID': new_id,\n                'text': aug_text,\n                'src': new_src,\n                'label': label\n            }\n            augmented_data.append(augmented_row)\n\n    return augmented_data  # список словарей\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T03:02:13.946709Z","iopub.execute_input":"2025-11-30T03:02:13.947276Z","iopub.status.idle":"2025-11-30T03:02:13.953660Z","shell.execute_reply.started":"2025-11-30T03:02:13.947254Z","shell.execute_reply":"2025-11-30T03:02:13.952993Z"}},"outputs":[],"execution_count":100},{"cell_type":"code","source":"# Аугментируем данные\nlength = len(train_data)\naugmented_data1 = augment_data_with_structure(train_data[:length//2], augmentation_factor=2)\n# augmented_data2 = augment_data_with_structure(train_data[length//2:], augmentation_factor=2)\n\n# Токенизация через твою функцию\ntokenized_augmented_dataset1 = tokenize_dataset(augmented_data1, tokenizer, max_length=MAX_LENGTH)\n# tokenized_augmented_dataset2 = tokenize_dataset(augmented_data2, tokenizer, max_length=MAX_LENGTH)\n\n# TensorDataset и DataLoader\nfrom torch.utils.data import TensorDataset, DataLoader\n\naugmented_dataset1 = TensorDataset(\n    tokenized_augmented_dataset1[\"input_ids\"],\n    tokenized_augmented_dataset1[\"attention_mask\"],\n    tokenized_augmented_dataset1[\"labels\"]\n)\n# augmented_dataset2 = TensorDataset(\n#     tokenized_augmented_dataset2[\"input_ids\"],\n#     tokenized_augmented_dataset2[\"attention_mask\"],\n#     tokenized_augmented_dataset2[\"labels\"]\n# )\n\naugmented_dataloader1 = DataLoader(augmented_dataset1, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n# augmented_dataloader2 = DataLoader(augmented_dataset2, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T03:16:29.698154Z","iopub.execute_input":"2025-11-30T03:16:29.698415Z","iopub.status.idle":"2025-11-30T03:29:19.859888Z","shell.execute_reply.started":"2025-11-30T03:16:29.698397Z","shell.execute_reply":"2025-11-30T03:29:19.859278Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Augmenting data:   0%|          | 0/116183 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6742afe0dd9426c81054176c6aa8ce1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing:   0%|          | 0/232366 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6117c2dbde9e4fa29cebb888bd0e6941"}},"metadata":{}}],"execution_count":105},{"cell_type":"code","source":"optimizer = optim.AdamW(model.parameters(), lr=1e-6)\nnum_training_steps = EPOCHS * len(train_dataloader)\nfrom transformers import get_cosine_schedule_with_warmup\n\nnum_training_steps = 1 * len(train_dataloader)\nnum_warmup_steps = int(0.1 * num_training_steps)\n\nscheduler = get_cosine_schedule_with_warmup(\n    optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T07:32:47.846601Z","iopub.execute_input":"2025-11-30T07:32:47.847074Z","iopub.status.idle":"2025-11-30T07:32:47.852245Z","shell.execute_reply.started":"2025-11-30T07:32:47.847052Z","shell.execute_reply":"2025-11-30T07:32:47.851476Z"}},"outputs":[],"execution_count":251},{"cell_type":"code","source":"checkpoint_path = \"checkpoint_step_540000.pt\"\n\n# Загружаем чекпоинт\ncheckpoint = torch.load(checkpoint_path, map_location=device)\n\n# Восстанавливаем модель\nmodel.load_state_dict(checkpoint['model_state_dict'])\nmodel.to(device)\n\n# Восстанавливаем оптимизатор и scheduler\noptimizer.load_state_dict(checkpoint['optimizer_state_dict'])\nscheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n\n# Восстанавливаем шаг и эпоху\nglobal_step = checkpoint['global_step']\nepoch = checkpoint['epoch']\nloss = checkpoint['loss']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T07:22:50.595781Z","iopub.execute_input":"2025-11-30T07:22:50.596066Z","iopub.status.idle":"2025-11-30T07:22:51.263230Z","shell.execute_reply.started":"2025-11-30T07:22:50.596046Z","shell.execute_reply":"2025-11-30T07:22:51.262659Z"}},"outputs":[],"execution_count":245},{"cell_type":"code","source":"accumulation_steps = 4  # число батчей для накопления градиентов\nglobal_step = 0\n\nmodel.train()\nfor epoch in range(1):\n    train_loss = 0\n    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\")\n\n    start_batch = 0\n    optimizer.zero_grad()  # обнуляем градиенты в начале эпохи\n\n    for i, batch in enumerate(progress_bar):\n        if i < start_batch: \n            continue\n            \n        input_ids, attention_mask, labels = [x.to(device) for x in batch]\n\n        outputs = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            labels=labels\n        )\n        loss = outputs.loss\n        loss = loss / accumulation_steps  # делим loss на accumulation_steps\n        loss.backward()\n        train_loss += loss.item() * accumulation_steps  # для логов возвращаем исходное значение\n\n        # обновляем веса каждые accumulation_steps батчей\n        if (i + 1) % accumulation_steps == 0:\n            optimizer.step()\n            scheduler.step()\n            optimizer.zero_grad()\n\n        progress_bar.set_postfix({\"loss\": train_loss / (i+1)})\n\n        global_step += BATCH_SIZE\n        \n        if (global_step % save_steps) == 0:\n            save_checkpoint(model, epoch, optimizer, scheduler, loss * accumulation_steps)\n\n    # Если число батчей не кратно accumulation_steps, обновляем оставшиеся градиенты\n    if (i + 1) % accumulation_steps != 0:\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n\n    print(f'Epoch {epoch+1} finished, avg train loss: {train_loss / len(train_dataloader):.4f}')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T07:34:17.651501Z","iopub.execute_input":"2025-11-30T07:34:17.652204Z","iopub.status.idle":"2025-11-30T07:35:23.673751Z","shell.execute_reply.started":"2025-11-30T07:34:17.652179Z","shell.execute_reply":"2025-11-30T07:35:23.672786Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Epoch 1:   0%|          | 0/6536 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4457e9e9fb74ecab587163e240a8358"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_132/2230889014.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0maccumulation_steps\u001b[0m  \u001b[0;31m# делим loss на accumulation_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0maccumulation_steps\u001b[0m  \u001b[0;31m# для логов возвращаем исходное значение\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# обновляем веса каждые accumulation_steps батчей\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":254},{"cell_type":"code","source":"save_checkpoint(model, epoch, optimizer, scheduler, loss)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T22:52:58.616040Z","iopub.execute_input":"2025-11-29T22:52:58.616316Z","iopub.status.idle":"2025-11-29T22:52:59.619840Z","shell.execute_reply.started":"2025-11-29T22:52:58.616295Z","shell.execute_reply":"2025-11-29T22:52:59.618996Z"}},"outputs":[{"name":"stdout","text":"\nCheckpoint saved at step 0 → checkpoint_step_0.pt\n","output_type":"stream"}],"execution_count":73},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T05:59:01.140798Z","iopub.execute_input":"2025-11-30T05:59:01.141579Z","iopub.status.idle":"2025-11-30T05:59:01.145448Z","shell.execute_reply.started":"2025-11-30T05:59:01.141552Z","shell.execute_reply":"2025-11-30T05:59:01.144670Z"}},"outputs":[{"name":"stdout","text":"1\n","output_type":"stream"}],"execution_count":164},{"cell_type":"code","source":"from sklearn.metrics import f1_score\nfrom tqdm.auto import tqdm\nimport torch\n\ndef evaluate_model(model, dataloader, device, not_more=3000):\n    model.eval()\n    all_predictions = []\n    all_labels = []\n\n    for batch in tqdm(dataloader, desc=\"Evaluating\"):\n        # Распаковываем батч и отправляем на устройство\n        input_ids, attention_mask, labels = [x.to(device) for x in batch]\n\n        # Отключаем градиенты для inference\n        with torch.no_grad():\n            outputs = model(\n                input_ids=input_ids,\n                attention_mask=attention_mask\n            )\n\n        # Берём логиты и делаем argmax по последней размерности\n        logits = outputs.logits\n        predictions = torch.argmax(logits, dim=-1)\n\n        # Сохраняем предсказания и настоящие метки на CPU\n        all_predictions.extend(predictions.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\n        if len(all_predictions) > not_more:\n            break\n\n    # Вычисляем F1-macro\n    f1_macro = f1_score(all_labels, all_predictions, average='macro')\n\n    return f1_macro, all_predictions, all_labels\n\nf1, preds, labels = evaluate_model(model, val_dataloader, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T07:35:28.963906Z","iopub.execute_input":"2025-11-30T07:35:28.964210Z","iopub.status.idle":"2025-11-30T07:36:14.718688Z","shell.execute_reply.started":"2025-11-30T07:35:28.964188Z","shell.execute_reply":"2025-11-30T07:36:14.718088Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/727 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d21c437b13f4c82a81328d6bb1df774"}},"metadata":{}}],"execution_count":255},{"cell_type":"code","source":"print(f1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T07:36:20.873273Z","iopub.execute_input":"2025-11-30T07:36:20.873844Z","iopub.status.idle":"2025-11-30T07:36:20.878209Z","shell.execute_reply.started":"2025-11-30T07:36:20.873819Z","shell.execute_reply":"2025-11-30T07:36:20.877323Z"}},"outputs":[{"name":"stdout","text":"0.7148290202365025\n","output_type":"stream"}],"execution_count":257},{"cell_type":"code","source":"model.to(device)\nmodel.eval()\ntorch.save(model.state_dict(), \"/kaggle/working/full_model_weights1.pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T06:10:47.051700Z","iopub.execute_input":"2025-11-30T06:10:47.052189Z","iopub.status.idle":"2025-11-30T06:10:48.595317Z","shell.execute_reply.started":"2025-11-30T06:10:47.052167Z","shell.execute_reply":"2025-11-30T06:10:48.594484Z"}},"outputs":[],"execution_count":176},{"cell_type":"code","source":"!pip install -q PyDrive2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T06:13:42.204830Z","iopub.execute_input":"2025-11-30T06:13:42.205441Z","iopub.status.idle":"2025-11-30T06:13:46.985566Z","shell.execute_reply.started":"2025-11-30T06:13:42.205405Z","shell.execute_reply":"2025-11-30T06:13:46.984592Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m81.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":179},{"cell_type":"code","source":"import shutil\nshutil.move(\"./full_model_weights1.pt\", \"/kaggle/working/full_model_weights1.pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T06:24:20.962099Z","iopub.execute_input":"2025-11-30T06:24:20.962880Z","iopub.status.idle":"2025-11-30T06:24:20.967870Z","shell.execute_reply.started":"2025-11-30T06:24:20.962855Z","shell.execute_reply":"2025-11-30T06:24:20.967302Z"}},"outputs":[{"execution_count":208,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/full_model_weights1.pt'"},"metadata":{}}],"execution_count":208},{"cell_type":"code","source":"ls","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T06:59:37.386837Z","iopub.execute_input":"2025-11-30T06:59:37.387593Z","iopub.status.idle":"2025-11-30T06:59:37.738337Z","shell.execute_reply.started":"2025-11-30T06:59:37.387570Z","shell.execute_reply":"2025-11-30T06:59:37.737652Z"}},"outputs":[{"name":"stdout","text":"checkpoint_step_120000.pt  checkpoint_step_420000.pt  full_model_weights.pt\ncheckpoint_step_180000.pt  checkpoint_step_480000.pt  \u001b[0m\u001b[01;34mhack-change-2025\u001b[0m/\ncheckpoint_step_240000.pt  checkpoint_step_540000.pt  \u001b[01;34mkaggle\u001b[0m/\ncheckpoint_step_300000.pt  checkpoint.zip             \u001b[01;34mrubert_local\u001b[0m/\ncheckpoint_step_360000.pt  full_model_weights1.pt     save_data_new.pkl\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":229},{"cell_type":"code","source":"!zip checkpoint.zip full_model_weights1.pt ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T06:34:04.666403Z","iopub.execute_input":"2025-11-30T06:34:04.667372Z","iopub.status.idle":"2025-11-30T06:34:42.326725Z","shell.execute_reply.started":"2025-11-30T06:34:04.667338Z","shell.execute_reply":"2025-11-30T06:34:42.325762Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"  adding: full_model_weights1.pt (deflated 7%)\n","output_type":"stream"}],"execution_count":221},{"cell_type":"code","source":"!cp /kaggle/working/full_model_weights1.pt  /content/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T06:39:18.915036Z","iopub.execute_input":"2025-11-30T06:39:18.915351Z","iopub.status.idle":"2025-11-30T06:39:19.839625Z","shell.execute_reply.started":"2025-11-30T06:39:18.915324Z","shell.execute_reply":"2025-11-30T06:39:19.838856Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":223},{"cell_type":"code","source":"from google.colab import drive\ndrive.mount('/content/drive')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T06:42:31.048021Z","iopub.execute_input":"2025-11-30T06:42:31.048893Z","iopub.status.idle":"2025-11-30T06:42:31.109766Z","shell.execute_reply.started":"2025-11-30T06:42:31.048862Z","shell.execute_reply":"2025-11-30T06:42:31.108747Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_132/1408506528.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    116\u001b[0m   \u001b[0;34m\"\"\"Internal helper to mount Google Drive.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/var/colab/hostname'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m     raise NotImplementedError(\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0;34m'Mounting drive is unsupported in this environment. Use PyDrive2'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;34m' instead. See examples at'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNotImplementedError\u001b[0m: Mounting drive is unsupported in this environment. Use PyDrive2 instead. See examples at https://colab.research.google.com/notebooks/io.ipynb#scrollTo=7taylj9wpsA2."],"ename":"NotImplementedError","evalue":"Mounting drive is unsupported in this environment. Use PyDrive2 instead. See examples at https://colab.research.google.com/notebooks/io.ipynb#scrollTo=7taylj9wpsA2.","output_type":"error"}],"execution_count":225},{"cell_type":"code","source":"import os -> os.chdir(r'/kaggle/working')","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-30T07:36:29.719Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'full_model_weights1.pt')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T06:59:04.231552Z","iopub.execute_input":"2025-11-30T06:59:04.232286Z","iopub.status.idle":"2025-11-30T06:59:04.236762Z","shell.execute_reply.started":"2025-11-30T06:59:04.232266Z","shell.execute_reply":"2025-11-30T06:59:04.236188Z"}},"outputs":[{"execution_count":228,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/full_model_weights1.pt","text/html":"<a href='full_model_weights1.pt' target='_blank'>full_model_weights1.pt</a><br>"},"metadata":{}}],"execution_count":228},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}